{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try some quick setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Lambda, Layer\n",
    "from keras.initializers import Constant\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "# Custom loss layer\n",
    "class CustomMultiLossLayer(Layer):\n",
    "    def __init__(self, nb_outputs=2, **kwargs):\n",
    "        self.nb_outputs = nb_outputs\n",
    "        self.is_placeholder = True\n",
    "        super(CustomMultiLossLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape=None):\n",
    "        # initialise log_vars\n",
    "        self.log_vars = []\n",
    "        for i in range(self.nb_outputs):\n",
    "            self.log_vars += [self.add_weight(name='log_var' + str(i), shape=(1,),\n",
    "                                              initializer=Constant(0.), trainable=True)]\n",
    "        super(CustomMultiLossLayer, self).build(input_shape)\n",
    "\n",
    "    def multi_loss(self, ys_true, ys_pred):\n",
    "        assert len(ys_true) == self.nb_outputs and len(ys_pred) == self.nb_outputs\n",
    "        loss = 0\n",
    "        for y_true, y_pred, log_var in zip(ys_true, ys_pred, self.log_vars):\n",
    "            precision = K.exp(-log_var[0])\n",
    "            loss += K.sum(precision * (y_true - y_pred)**2. + log_var[0], -1)\n",
    "        return K.mean(loss)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        ys_true = inputs[:self.nb_outputs]\n",
    "        ys_pred = inputs[self.nb_outputs:]\n",
    "        loss = self.multi_loss(ys_true, ys_pred)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # We won't actually use the output.\n",
    "        return K.concatenate(inputs, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull in raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.concat([pd.read_csv(\"/home/ms994/beat_pd/data/real-pd/data_labels/REAL-PD_Training_Data_IDs_Labels.csv\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(591, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.dropna().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_specific = labels.groupby(\"subject_id\").mean().dropna()\n",
    "all_m_id = labels.measurement_id.unique().tolist()\n",
    "train_ind, test_ind = train_test_split([i for i in range(len(all_m_id))], test_size=0.1, random_state=1)\n",
    "train_ind, valid_ind = train_test_split(train_ind,  test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "allResults = pkl.load(open(\"/n/scratch2/ms994/realPd.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63262"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ms994/miniconda3/envs/keras-redo-cpu/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in greater\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "allResults = list(filter(lambda x: x[0].shape[0] >= 1500, allResults))\n",
    "allResults = list(filter(lambda x: x[1].shape[0] >= 1500, allResults))\n",
    "allResults = list(filter(lambda x: x[2].shape[0] >= 1500, allResults))\n",
    "allResults = (list(filter(lambda x: (x[0].std(0) > 0.01).any() and (x[1].std(0) > 0.01).any() and (x[2].std(0) > 0.01).any(), allResults)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41079"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [datum[3] for datum in allResults]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.concat(labels, axis=1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.reset_index().drop([\"index\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subjects = labels[\"subject_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    num_cnn_layers = 5\n",
    "    num_lstm_layers = 1\n",
    "    num_lin_layers = 5\n",
    "    dropout = 0.5\n",
    "    lin_h=64\n",
    "\n",
    "\n",
    "\n",
    "    watch_accel = keras.layers.Input((1500, 3), name=\"watch_accel_in\")\n",
    "    watch_gyro = keras.layers.Input((1500, 3), name=\"watch_gyro_in\")\n",
    "    phone_accel = keras.layers.Input((1500, 3), name=\"phone_accel_in\")\n",
    "    all_inputs = [watch_accel, watch_gyro, phone_accel]\n",
    "    all_cnn_outputs = []\n",
    "\n",
    "    for inputLayer in all_inputs:\n",
    "        x = inputLayer\n",
    "    #     x = keras.layers.GaussianNoise(0.1)(x)\n",
    "\n",
    "\n",
    "        for i in range(num_cnn_layers):\n",
    "            x = keras.layers.Conv1D(2**int(min(i, 4))*3, (3,), padding=\"same\")(x)\n",
    "            x = keras.layers.LeakyReLU()(x)\n",
    "            x = keras.layers.MaxPool1D((2,))(x)\n",
    "            x = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "        x = keras.layers.Flatten()(x)\n",
    "        all_cnn_outputs.append(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    x = keras.layers.Concatenate()(all_cnn_outputs)\n",
    "    x = keras.layers.Dense(lin_h)(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "    x_shared_flattened = x\n",
    "\n",
    "    #one_off\n",
    "    x = x_shared_flattened \n",
    "    for k in range(num_lin_layers):\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.Dense(lin_h)(x)\n",
    "        x = keras.layers.LeakyReLU()(x)\n",
    "        x = keras.layers.Dropout(dropout)(x)\n",
    "    x = keras.layers.Dense(1)(x)\n",
    "    x_on_off = keras.layers.ReLU(name=\"out\", max_value=1)(x)\n",
    "\n",
    "    model = keras.Model(inputs=all_inputs, outputs=[x_on_off])\n",
    "    model.compile(keras.optimizers.Adam(lr=0.001), loss=\"mean_squared_error\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from addict import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0510 17:21:18.477278 139989104256832 module_wrapper.py:139] From /home/ms994/miniconda3/envs/keras-redo-cpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0510 17:21:18.480878 139989104256832 module_wrapper.py:139] From /home/ms994/miniconda3/envs/keras-redo-cpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0510 17:21:18.481714 139989104256832 module_wrapper.py:139] From /home/ms994/miniconda3/envs/keras-redo-cpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0510 17:21:18.496351 139989104256832 module_wrapper.py:139] From /home/ms994/miniconda3/envs/keras-redo-cpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0510 17:21:18.547845 139989104256832 module_wrapper.py:139] From /home/ms994/miniconda3/envs/keras-redo-cpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0510 17:21:20.003207 139989104256832 deprecation.py:506] From /home/ms994/miniconda3/envs/keras-redo-cpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0510 17:21:20.433749 139989104256832 module_wrapper.py:139] From /home/ms994/miniconda3/envs/keras-redo-cpu/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0510 17:21:20.527235 139989104256832 deprecation.py:323] From /home/ms994/miniconda3/envs/keras-redo-cpu/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0510 17:21:22.608235 139989104256832 module_wrapper.py:139] From /home/ms994/miniconda3/envs/keras-redo-cpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0510 17:21:23.252712 139989104256832 module_wrapper.py:139] From /home/ms994/miniconda3/envs/keras-redo-cpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "W0510 17:21:24.092557 139989104256832 module_wrapper.py:139] From /home/ms994/miniconda3/envs/keras-redo-cpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0510 17:21:24.099503 139989104256832 module_wrapper.py:139] From /home/ms994/miniconda3/envs/keras-redo-cpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0510 17:21:24.100033 139989104256832 module_wrapper.py:139] From /home/ms994/miniconda3/envs/keras-redo-cpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:184: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3688 samples, validate on 462 samples\n",
      "Epoch 1/500\n"
     ]
    }
   ],
   "source": [
    "m_id = []\n",
    "testScores = Dict()\n",
    "for subject in all_subjects[:1]:\n",
    "    watchAccel = []\n",
    "    watchGyro = []\n",
    "    phoneAccel = []\n",
    "    allLabels = []\n",
    "    label = labels[labels[\"subject_id\"] == subject]\n",
    "    for i in label.index:\n",
    "        res = allResults[i]\n",
    "        watchAccel.append(res[0][:1500])\n",
    "        watchGyro.append(res[1][:1500])\n",
    "        phoneAccel.append(res[2][:1500])\n",
    "        allLabels.append(res[3])\n",
    "    allLabels = pd.concat(allLabels, axis=1).T\n",
    "    allLabels = allLabels.reset_index().drop([\"index\"], 1)\n",
    "    assert allLabels.subject_id.unique() == subject\n",
    "    m_id.append(allLabels.measurement_id.unique())\n",
    "    train, test = train_test_split(labels[labels.subject_id == subject].measurement_id.unique(), test_size=0.1, random_state=1)\n",
    "    train, valid = train_test_split(train, test_size=0.1, random_state=1)\n",
    "\n",
    "    train_labels = allLabels[allLabels.measurement_id.apply(lambda m_id: m_id in train)]\n",
    "    valid_labels = allLabels[allLabels.measurement_id.apply(lambda m_id: m_id in valid)]\n",
    "    test_labels = allLabels[allLabels.measurement_id.apply(lambda m_id: m_id in test)]\n",
    "    train_data = [np.array(watchAccel)[train_labels.index], np.array(watchGyro)[train_labels.index], np.array(phoneAccel)[train_labels.index]]\n",
    "    valid_data = [np.array(watchAccel)[valid_labels.index], np.array(watchGyro)[valid_labels.index], np.array(phoneAccel)[valid_labels.index]]\n",
    "    test_data = [np.array(watchAccel)[test_labels.index], np.array(watchGyro)[test_labels.index], np.array(phoneAccel)[test_labels.index]]\n",
    "\n",
    "\n",
    "    for output in [\"on_off\", \"dyskinesia\", \"tremor\"]:\n",
    "        model = get_model()\n",
    "        weighting = allLabels[output].mean()\n",
    "        modelCheckpoint = keras.callbacks.ModelCheckpoint(f\"/n/scratch2/ms994/cnn_on_off_s_id_{subject}_{output}.h5\", save_best_only=True, verbose=True)\n",
    "        earlyStopping = keras.callbacks.EarlyStopping(patience=20)\n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=2)\n",
    "        history = model.fit(train_data, train_labels[output], epochs=500, batch_size=32, validation_data=(valid_data, valid_labels[output]), callbacks=[modelCheckpoint, reduce_lr, earlyStopping])\n",
    "        plt.plot(history.history[\"loss\"])\n",
    "        plt.plot(history.history[\"val_loss\"])\n",
    "        plt.legend([\"Train\", \"Valid\"])\n",
    "        plt.show()\n",
    "        testScores[subject][output] = model.evaluate(test_data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_data, train_labels[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isnull(allLabels.).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[labels[\"subject_id\"] == a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mid = labels.measurement_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mid = all_mid.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(all_data, indices, std_threshold, column):\n",
    "    possible_match = []\n",
    "    for i, datum in enumerate(all_data):\n",
    "        \n",
    "        if all_mid.index(datum[3].measurement_id) in indices \\\n",
    "            and (datum[0].std(0) > std_threshold).any() \\\n",
    "            and (datum[1].std(0) > std_threshold).any() \\\n",
    "            and (datum[2].std(0) > std_threshold).any() \\\n",
    "            and (datum[0].shape[0] >= 1500) \\\n",
    "            and (datum[1].shape[0] >= 1500) \\\n",
    "            and (datum[2].shape[0] >= 1500) \\\n",
    "            and (not np.isnan(datum[3][column])):\n",
    "            \n",
    "            possible_match.append((datum[0][:1500], datum[1][:1500], datum[2][:1500], datum[3]))\n",
    "    labels = []\n",
    "    watch_accel = []\n",
    "    watch_gyro = []\n",
    "    phone_accel = []\n",
    "    for datum in possible_match:\n",
    "        watch_accel.append(datum[0][:1500])\n",
    "        watch_gyro.append(datum[1][:1500])\n",
    "        phone_accel.append(datum[2][:1500])\n",
    "        labels.append(datum[3][column])\n",
    "    \n",
    "    return np.array(watch_accel), np.array(watch_gyro), np.array(phone_accel), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = prep_data(allResults, train_ind, 0.01, \"on_off\")\n",
    "valid_data = prep_data(allResults, valid_ind, 0.01, \"on_off\")\n",
    "test_data = prep_data(allResults, test_ind, 0.01, \"on_off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datum[3][\"on_off\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data[0]), len(valid_data[0]), len(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watch_accel = []\n",
    "watch_gyro = []\n",
    "phone_accel = []\n",
    "on_off = []\n",
    "dyskinesia = []\n",
    "tremor = []\n",
    "subject = []\n",
    "for datum in train_data:\n",
    "    watch_accel.append(datum[0][:1500])\n",
    "    watch_gyro.append(datum[1][:1500])\n",
    "    phone_accel.append(datum[2][:1500])\n",
    "    on_off.append(datum[3].on_off)\n",
    "    dyskinesia.append(datum[3].dyskinesia)\n",
    "    tremor.append(datum[3].tremor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watch_accel[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(watch_accel).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(watch_gyro).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cnn_layers = 5\n",
    "num_lstm_layers = 0\n",
    "num_lin_layers = 5\n",
    "dropout = 0.5\n",
    "lin_h=128\n",
    "watch_accel = keras.layers.Input((1500, 3), name=\"watch_accel_in\")\n",
    "watch_gyro = keras.layers.Input((1500, 3), name=\"watch_gyro_in\")\n",
    "phone_accel = keras.layers.Input((1500, 3), name=\"phone_accel_in\")\n",
    "all_inputs = [watch_accel, watch_gyro, phone_accel]\n",
    "all_cnn_outputs = []\n",
    "\n",
    "for inputLayer in all_inputs:\n",
    "    x = inputLayer\n",
    "    x = keras.layers.GaussianNoise(0.1)(x)\n",
    "\n",
    "\n",
    "    for i in range(num_cnn_layers):\n",
    "        x = keras.layers.Conv1D(2**int(min(i, 3))*3, (3,), padding=\"same\")(x)\n",
    "        x = keras.layers.LeakyReLU()(x)\n",
    "        x = keras.layers.MaxPool1D((2,))(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    all_cnn_outputs.append(x)\n",
    "\n",
    "x = keras.layers.Concatenate()(all_cnn_outputs)\n",
    "\n",
    "x = keras.layers.Dense(lin_h)(x)\n",
    "x = keras.layers.LeakyReLU()(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "x_shared_flattened = x\n",
    "\n",
    "#one_off\n",
    "x = x_shared_flattened \n",
    "for k in range(num_lin_layers):\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dense(lin_h)(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    x = keras.layers.Dropout(dropout)(x)\n",
    "x = keras.layers.Dense(1)(x)\n",
    "x_on_off = keras.layers.ReLU(name=\"on_off\", max_value=1)(x)\n",
    "\n",
    "\n",
    "\n",
    "model = keras.Model(inputs=[watch_accel, watch_gyro, phone_accel], outputs=[x_on_off])\n",
    "model.compile(keras.optimizers.Adam(lr=0.001), loss=[\"mean_squared_error\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
